model:
  name: meta-llama/Llama-3.2-1B
  device_map: auto
  attn_implementation: eager
qlora:
  load_in_4bit: True
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: True
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
  target_modules: ["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"]
data: 
  train: data/GereralAI/train/my_data.csv
  num_proc: 4
trainer:
  dataset_text_field: text
  max_seq_length: 800
  dataset_num_proc: 2
  packing: True
training_args:
  learning_rate: 3.0e-4
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  logging_steps: 1
  optim: paged_adamw_32bit
  output_dir: output
  warmup_steps: 10
  logging_strategy: steps
  fp16: False
  bf16: False
  group_by_length: True
  report_to: wandb
hugginface:
  repo_name: ueihieu/pretrain_LLama