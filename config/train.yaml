model:
  name: meta-llama/Llama-3.2-1B
  device_map: auto
  attn_implementation: eager
qlora:
  load_in_4bit: True
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: True
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
  target_modules: ["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"]
data: 
  path: data/GereralAI/train/train.csv
  num_proc: 4
trainer:
  dataset_text_field: text
  max_seq_length: 3000
  dataset_num_proc: 2
  packing: True
training_args:
  learning_rate: 3.0e-4
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  num_train_epochs: 40
  logging_steps: 1
  optim: paged_adamw_32bit
  output_dir: output
  evaluation_strategy: epoch
  eval_steps: 0.2
  warmup_steps: 10
  logging_strategy: steps
  fp16: False
  bf16: False
  group_by_length: True
  report_to: wandb
  save_strategy: epoch
  save_total_limit: 5
  load_best_model_at_end: True