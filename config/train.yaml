model:
  name: meta-llama/Llama-3.2-1B
  max_seq_length: 4000
  load_in_4bit: True
  dtype: None
peft:
  r: 16
  lora_alpha: 16
  lora_dropout: 0
  target_modules: ["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"]
  use_rslora: True
  use_gradient_checkpointing: unsloth
  random_state: 32
  loftq_config: None
data: 
  prompt: |
    Answer this question:
    {}
    Answer:
    {}
  path: data/train/train.csv
trainer:
  dataset_text_field: text
  max_seq_length: 4000
  dataset_num_proc: 2
  packing: True
training_args:
  learning_rate: 3.0e-4
  lr_scheduler_type: linear
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 8
  num_train_epochs: 40
  logging_steps: 1
  optim: adamw_8bit
  weight_decay: 0.01
  warmup_steps: 10
  output_dir: output
  seed: 0